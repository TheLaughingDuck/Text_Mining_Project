{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrcIKalKeolw"
      },
      "source": [
        "# This Notebook...\n",
        "...loads data from \"data_raw.csv\" which was assembled in \"query.ipynb\", and then preprocesses the data to make it prepared for training the models. The preprocessing involves filtering, reformatting, construction of the tfidf-ebedding and the DistilBERT embedding, and splitting into train, valid, and test datasets. The final data are saved as three dictionaries \"data_y.pkl\", \"data_tfidf.pkl\", and \"data_DistilBERT.pkl\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8cyOeYteolz"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1FqnYClWeol0"
      },
      "outputs": [],
      "source": [
        "# Important\n",
        "#import utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# Helpers\n",
        "from itertools import islice\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from itertools import islice, product\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nZGKkAoeol2"
      },
      "source": [
        "# Load Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ewv6BKQ8eol2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data_raw.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwxpF8hAeol2"
      },
      "source": [
        "# Filter and modify the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EFLiHJJFeol2"
      },
      "outputs": [],
      "source": [
        "# Some reviews are float('nan'). These are turned into empty reviews (\"\").\n",
        "# There does not seem to be any special cases that are neither string nor float('nan'), but we put \"\" just in case.\n",
        "df[\"review\"] = [i if type(i) == str else (\"\" if math.isnan(i) else \"\") for i in df[\"review\"]]\n",
        "\n",
        "# Remove the empty reviews\n",
        "df = df[df[\"review\"] != \"\"]\n",
        "\n",
        "# Set aside the labels (helpfulness)\n",
        "y = df[\"helpfulness\"] >= 0.5\n",
        "df = df.drop([\"helpfulness\"], axis=1)\n",
        "\n",
        "# Drop indices\n",
        "df = df.reset_index(drop=True)\n",
        "y = y.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrtPjZmteol3"
      },
      "source": [
        "# Construct tfidf embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-ixCgQADeol3"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "matrix_tfidf = vectorizer.fit_transform(df['review'])\n",
        "matrix_tfidf = pd.DataFrame.sparse.from_spmatrix(matrix_tfidf) #\"Un\"-sparse the matrix\n",
        "\n",
        "# Combine df and the tfidf matrix\n",
        "X_tfidf = pd.concat([df.reset_index(drop=True), matrix_tfidf.reset_index(drop=True)], axis=1)\n",
        "X_tfidf = X_tfidf.drop([\"review\"], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0tARBY4eol3"
      },
      "source": [
        "# Construct DistilBERT embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA-gpSlcgm3o",
        "outputId": "c4e0dd02-149b-4f78-a294-d9b2df7b8254"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
            "100%|██████████| 35567/35567 [3:39:23<00:00,  2.70it/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Create a first X matrix\n",
        "#X_DistilBERT = pd.concat([df.drop([\"review\"], axis=1), pd.DataFrame({i:[] for i in range(768)})], axis=1)\n",
        "X_DistilBERT = pd.DataFrame({i:[float(0)] for i in range(768)})\n",
        "\n",
        "for review in tqdm(df[\"review\"]):\n",
        "    # Tokenize and embed\n",
        "    inputs = tokenizer(review, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Aggregate token embeddings to get sentence embeddings\n",
        "    sentence_embedding = tf.reduce_mean(input_tensor=outputs.last_hidden_state, axis=1)\n",
        "\n",
        "    # Combine df and the DistilBERT embeddings\n",
        "    X_DistilBERT.loc[X_DistilBERT.index[-1], :] = sentence_embedding.numpy()[0]\n",
        "\n",
        "X_DistilBERT = pd.concat([df.drop([\"review\"], axis=1), X_DistilBERT], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_ffvcy6Gdstg"
      },
      "outputs": [],
      "source": [
        "with open(\"X_DistilBERT.pkl\", \"wb\") as f:\n",
        "    pickle.dump(X_DistilBERT, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-kdz-qBeol3"
      },
      "outputs": [],
      "source": [
        "# from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "# import tensorflow as tf\n",
        "\n",
        "# # Load tokenizer and tokenize\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "# inputs = tokenizer(list(df[\"review\"]), padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "# # Load model and embed\n",
        "# model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "# outputs = model(inputs)\n",
        "\n",
        "# # Aggregate token embeddings to get sentence embeddings\n",
        "# sentence_embeddings = tf.reduce_mean(input_tensor=outputs.last_hidden_state, axis=1)\n",
        "\n",
        "# # Combine df and the DistilBERT embeddings\n",
        "# X_DistilBERT = pd.concat([df, pd.DataFrame(sentence_embeddings)], axis=1)\n",
        "# X_DistilBERT = X_DistilBERT.drop([\"review\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-Z7m7E1reol4"
      },
      "outputs": [],
      "source": [
        "# Verification check\n",
        "assert X_tfidf.shape[0] == X_DistilBERT.shape[0]\n",
        "assert X_tfidf.shape[0] == df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We-2xDL2eol5"
      },
      "source": [
        "# Train, validation and test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pWocJ8gSeol5"
      },
      "outputs": [],
      "source": [
        "# Randomise train, valid, and test indices\n",
        "n_observations = df.shape[0]\n",
        "indices = np.random.choice([i for i in range(n_observations)], size=n_observations, replace=False)\n",
        "\n",
        "# Extract train, valid, and test indices\n",
        "train_indices = indices[0:round(0.7*n_observations)]\n",
        "valid_indices = indices[(round(0.7*n_observations)):round(0.85*n_observations)]\n",
        "test_indices = indices[(round(0.85*n_observations)):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JJNuS2dzeol5"
      },
      "outputs": [],
      "source": [
        "# Save the labels (note: labels are the same for tfidf data and DistilBERT).\n",
        "data_y = {\"y_train\": y[train_indices], \"y_valid\": y[valid_indices], \"y_test\": y[test_indices]}\n",
        "with open(\"data_y.pkl\", \"wb\") as f:\n",
        "    pickle.dump(data_y, f)\n",
        "\n",
        "# Save tfidf predictors\n",
        "data_tfidf = {\"X_train\": X_tfidf[train_indices], \"X_valid\": X_tfidf[valid_indices], \"X_test\": X_tfidf[test_indices]}\n",
        "with open(\"data_tfidf.pkl\", \"wb\") as f:\n",
        "    pickle.dump(data_tfidf, f)\n",
        "\n",
        "# Save DistilBERT predictors\n",
        "data_distilBERT = {\"X_train\": X_DistilBERT.iloc[train_indices], \"X_valid\": X_DistilBERT.iloc[valid_indices], \"X_test\": X_DistilBERT.iloc[test_indices]}\n",
        "with open(\"data_DistilBERT.pkl\", \"wb\") as f:\n",
        "    pickle.dump(data_distilBERT, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tm_vnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
